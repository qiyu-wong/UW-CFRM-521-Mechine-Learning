{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<h1>\n",
    "<center>CFRM 521, Spring 2020</center>\n",
    "</h1>\n",
    "\n",
    "<h1>\n",
    "<center>[Please insert your name(s) here]</center>\n",
    "</h1>\n",
    "\n",
    "<h1>\n",
    "<center>Homework 2</center>\n",
    "</h1>\n",
    "\n",
    "* **Due: Monday 4th May 2020, at 11:59pm**\n",
    "\n",
    "* You may submitted this homework with one other classmate. If you are submitting in a group of two, only one of you must submit the homework. The other must make a comment on Canvas (where you would submit the homework) mentioning that they are submitting with another person. Also, please include both of your names in the homework submission.\n",
    "\n",
    "\n",
    "* Late homework are allowed, **but a 10% penalty per day applies.** Example: submitting on Tuesday 00:01am results in 10% penalty, on Wednesday 00:01am results in 20% penalty, etc. Your last submission is considered for calculating the penalty.\n",
    "\n",
    "\n",
    "*  Please use this Jupyter notebook as a template for your solutions. Your solution must be submitted as one Jupyter notebook. You are allowed to use code from the textbook, textbook website, or lecture notes. However, please go over the code you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.\n",
    "## (a)\n",
    "Derive the dual formulation of soft margin support vector machine. Show your steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)\n",
    "Read the section \"SVM Regression\" in Chapter 5 of the textbook. For further information, refer to [scikit-learn documentation](https://scikit-learn.org/stable/modules/svm.html#svm-implementation-details) and [this article](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=97DEC888B5D7A6B3DECBA36B29BED35E?doi=10.1.1.114.4288&rep=rep1&type=pdf). Formulate a QP for hard margin form of the SVM regression. Also, formulate a QP for soft margin form of the SVM regression. You should explain your derivations. Writing down equations is not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. \n",
    "\n",
    "## (a)\n",
    "\n",
    "Solve Exercise 9 of Chapter 5 of the textbook. Report your 10-fold cross validation accuracy on the training set as well as your accuracy on the test set.\n",
    "\n",
    "**Hint:** Use a smaller subset of the training data (try to stratify it according to the digits) for training and grid search if doing these tasks takes a long time on the whole data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)\n",
    "\n",
    "Solve Exercise 10 of Chapter 5 of the textbook. Try experimenting with various kernels using the `SVR` class. Report your 10-fold cross validation RMSE on the training set as well as the RMSE on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\n",
    "## (a)\n",
    "Load the MNIST dataset and split into a training set (the first 50,000 observations), a validation set (the next 10,000 observations), and a test set (the last 10,000 observations). Train the following classifiers on the training set: 1) A random forest. 2) An Extra-Trees classifier. 3) An AdaBoost classifier. 4) A gradient boosting classifier. Report the accuracy of each trained classifier on the validation set.\n",
    "\n",
    "**Hint:** You may use the default values of the hyperparameters. Alternatively, you may fine-tune each model using a smaller training set (this step is not necessary for getting the points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)\n",
    "Train a hard-voting and a soft-voting ensemble classifiers based on the models from part (a). Evaluate each voting classifier on the validation set. Comment on whether the performance of the ensemble model is better or worst than the models in part (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)\n",
    "Evaluate all the models in part (a) and (b) on the test set. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. \n",
    "In Question 3, we have used predetermined rules (that is, hard-voting and soft-voting) to build the ensemble prediction. **Stacking** is an ensemble method in which you train a model (called a **blender**) for the task of how to aggregate the result of each predictor into the ensemble prediction. In this question, we build on the setting of Question 3 (i.e. training set, validation set, test sets, predictors, etc. are the same).\n",
    "\n",
    "# (a)\n",
    "For each model in Question 3.(a), make 50,000 **clean predictions** on the training set using 3-fold cross validation. You should end up with four predictions per observation.\n",
    "\n",
    "**Hint:** Use `sklearn.model_selection.cross_val_predict()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)\n",
    "Use the predictions in (a) as features and the actual label of the observations as the target.  So, your training set is 50,000 observations with 4 features per observation. Train a random forest classifier on this training set. This classifier is a blender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)\n",
    "Retrain the classifiers from Question 3.(a) on the whole training set. Now, make prediction on the validation set using the classifiers. Feed the resulting predictions into the blender you trained in part (b) and obtain the final prediction.  **Do not retrain the blender.** These are called stacking predictions. Report the accuracy of your stacking predictions on the validation set, and compare with what you found in Question 3.(b). Repeat this process for the test set and compare with what you found in Question 3.(c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.\n",
    "\n",
    "## (a)\n",
    "\n",
    "Train a SVM classifier with RBF kernel on the MNIST training data (the first 60,000 observations). It will take a long time if you use all the training data. Take a subset (for example 5,000 observations) such that training takes roughly 2 minutes on your machine. Time your training (use `%time`) and report the accuracy of the trained model on the test set (the last 10,000 observations of MNIST). Next, use PCA to find principal components with explained variance ratio of 95%. Retrain the SVM classifier using this principal components instead of the original features. Time the training and accuracy on the test set. Comment on whether PCA helped or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)\n",
    "\n",
    "Repeat part (a), but this time use a random forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
